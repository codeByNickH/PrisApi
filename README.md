# PrisApi - Swedish Grocery Price Scraper

> **üìö Educational Project**  
> This project was created for learning purposes to demonstrate web scraping, .NET development, and database design skills. It is not intended for commercial use or production deployment. See [Compliance Status](#Ô∏è-compliance-status) for details.

A comprehensive ASP.NET Core Web API demonstrating web scraping, browser automation, and database design skills through a grocery price tracking system.

## üéØ What It Does

Automates collection of product pricing data from four Swedish grocery retailers (ICA, Willys, Coop, City Gross), handling complex pricing scenarios like multi-buy discounts, member prices, and unit conversions.

Send some selected products to discord if the price changes. 

## üõ†Ô∏è Technical Stack

- **Backend**: ASP.NET Core 8, Entity Framework Core 9, SQL Server
- **Web Scraping**: Microsoft Playwright (headless browser automation)
- **Architecture**: Repository Pattern, Dependency Injection, Service Layer
- **Data Processing**: JSON parsing, regex, unit normalization (g‚Üíkg, ml‚Üíl)

## üèóÔ∏è Key Technical Features

**Real-time Notification Engine**
- Aggregates all price changes into a single summary payload to avoid API rate limits and notification spam.
- Visualizes price movement (üìâ DOWN / üìà UP / üÜï NEW) using a custom DTO logic.
- Sends error message if there is a problem with the data collection.

**Dynamic Configuration System**
- Database-stored scraper configurations (no hardcoded selectors)
- Easy adaptation to website changes without redeployment

**Complex Price Parsing**
```csharp
// Handles: "3 f√∂r 50kr", member pricing, quantity limits
// Normalizes units across different formats
// Tracks price history for trend analysis
```

**Intelligent Data Processing**
- Smart product matching to avoid duplicates
- Handles 10+ pricing scenarios (multi-buy, member-only, limits)
- Unit conversions and price normalization

**Database Design**
- Products with price history (time-series tracking)
- Many-to-many category relationships
- Store locations and configurations
- Scraping job metadata

## üìä Technical Specs

- 90 products per category (limited scope testing)
- 18 product categories √ó 4 stores
- 25+ store locations across Sweden
- 10-second request delays (respectful automation)

## üí° What I Learned (The Honest Version)

**Initial Approach**: Built this to learn web scraping and .NET. Like many beginners, I dove straight into implementation and tested against live sites.

**The Learning Moment**: Later discovered I should have checked robots.txt and Terms of Service *first*. This taught me a crucial lesson about responsible automation.

**Improvements Made**:
- ‚úÖ Increased delays from 50ms ‚Üí 10 seconds
- ‚úÖ Limited scope from comprehensive scraping ‚Üí 90 products/category
- ‚úÖ Researched robots.txt compliance for all target sites
- ‚úÖ Repositioned as educational project only

**Key Takeaway**: Technical capability must be balanced with legal and ethical responsibility. I now research compliance requirements before writing the first line of code for any automation project.

## üéì Skills Demonstrated

- **Backend Development**: RESTful API design, async/await patterns
- **Database Design**: Complex relationships, EF Core migrations
- **Software Architecture**: SOLID principles, separation of concerns. Implemented the Options Pattern to strongly type configuration settings and decouple services from the configuration system.
- **Security**: Secure management of sensitive credentials using .NET User Secrets and environment variables.
- **System Integration**: Asynchronous communication with external APIs (Discord Webhooks) using batched payloads.
- **Data Processing**: String parsing, unit conversion, duplicate detection
- **Web Scraping**: Browser automation, API interception, dynamic content handling
- **Problem-Solving**: Adapting approach based on real-world constraints
- **Professional Judgment**: Understanding when and how to build responsibly

## ‚öñÔ∏è Compliance Status

**Current Implementation**:
- robots.txt compliant (10-second delays, verified allowed paths)
- Limited educational testing (90 products/category)
- No commercial use or data resale
- Not deployed to production

**What This Would Need for Production**:
- Explicit written permission from each retailer
- Official API access or data partnership agreements
- Commercial licensing arrangements

## üíº Why This Makes Me a Strong Candidate

**Technical Skills**: Built a complex, multi-layered system demonstrating proficiency across the .NET ecosystem.

**Learning & Growth**: Recognized compliance issues independently and made significant improvements.

**Professional Maturity**: Understand the difference between technical capability and ethical responsibility.

**Problem-Solving**: Can articulate trade-offs, identify issues, and implement solutions.

---

**Development Note**: Like many developers learning web scraping, I initially tested this without fully understanding compliance requirements. After researching robots.txt and ToS, I made significant improvements to rate limiting and scope. This experience taught me to consider legal/ethical requirements from day one of any project - a lesson that makes me a more thoughtful developer.

*Status: Educational portfolio piece demonstrating technical skills and professional growth.*